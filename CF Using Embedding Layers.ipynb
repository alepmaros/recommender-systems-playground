{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094f40f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Add, Activation, Lambda, BatchNormalization, Concatenate, Dropout, Input, Embedding, Dot, Reshape, Dense, Flatten\n",
    "\n",
    "import tensorflow as tf\n",
    "# Inspirations:\n",
    "# https://www.kaggle.com/code/chaitanya99/recommendation-system-cf-anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44239b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d42e3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUR_INPUT = \"/mnt/d/datasets/anime2020/\"\n",
    "INPUT_DIR = \"/mnt/d/datasets/anime2020/animelist_400+/*.parquet\"\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823ab76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = pd.read_parquet(\n",
    "    glob.glob(INPUT_DIR)[0],\n",
    "    columns=[\"user\", \"anime\", \"rating\"]\n",
    ")\n",
    "\n",
    "rating_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c214c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = rating_df[\"user\"].nunique()\n",
    "n_animes = rating_df[\"anime\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a40f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df.memory_usage(index=False, deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1bb130",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Avg', np.mean(rating_df['rating']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543a2f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rating_df[['user', 'anime']].values\n",
    "y = rating_df[\"rating\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1c075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "test_set_size = 10000 #10k for test set\n",
    "train_indices = rating_df.shape[0] - test_set_size \n",
    "\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    X[:train_indices],\n",
    "    X[train_indices:],\n",
    "    y[:train_indices],\n",
    "    y[train_indices:],\n",
    ")\n",
    "\n",
    "print('> Train set ratings: {}'.format(len(y_train)))\n",
    "print('> Test set ratings: {}'.format(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee422911",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_array = [X_train[:, 0], X_train[:, 1]]\n",
    "X_test_array = [X_test[:, 0], X_test[:, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59473da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del rating_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bcaec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding layers\n",
    "\n",
    "def RecommenderNet():\n",
    "    embedding_size = 64\n",
    "    \n",
    "    user = Input(name = 'user', shape = [1])\n",
    "    user_embedding = Embedding(name = 'user_embedding',\n",
    "                               input_dim = n_users, \n",
    "                               output_dim = embedding_size)(user)\n",
    "    \n",
    "    anime = Input(name = 'anime', shape = [1])\n",
    "    anime_embedding = Embedding(name = 'anime_embedding',\n",
    "                                input_dim = n_animes, \n",
    "                                output_dim = embedding_size)(anime)\n",
    "    \n",
    "    #x = Concatenate()([user_embedding, anime_embedding])\n",
    "    x = Dot(name = 'dot_product', normalize = True, axes = 2)([user_embedding, anime_embedding])\n",
    "    x = Flatten()(x)\n",
    "        \n",
    "    x = Dense(1, kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=[user, anime], outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', metrics=[\"mae\", \"mse\"], optimizer='Adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = RecommenderNet()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5bc05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "start_lr = 0.00001\n",
    "min_lr = 0.00001\n",
    "max_lr = 0.00005\n",
    "batch_size = 1000\n",
    "\n",
    "rampup_epochs = 5\n",
    "sustain_epochs = 0\n",
    "exp_decay = .8\n",
    "\n",
    "def lrfn(epoch):\n",
    "    if epoch < rampup_epochs:\n",
    "        return (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n",
    "    elif epoch < rampup_epochs + sustain_epochs:\n",
    "        return max_lr\n",
    "    else:\n",
    "        return (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n",
    "\n",
    "\n",
    "lr_callback = LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=0)\n",
    "\n",
    "checkpoint_filepath = './weights.h5'\n",
    "\n",
    "model_checkpoints = ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                                        save_weights_only=True,\n",
    "                                        monitor='val_loss',\n",
    "                                        mode='min',\n",
    "                                        save_best_only=True)\n",
    "\n",
    "early_stopping = EarlyStopping(patience = 3, monitor='val_loss', \n",
    "                               mode='min', restore_best_weights=True)\n",
    "\n",
    "my_callbacks = [\n",
    "    model_checkpoints,\n",
    "    lr_callback,\n",
    "    early_stopping,   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f37e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x=X_train_array,\n",
    "    y=y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=20,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test_array, y_test),\n",
    "    callbacks=my_callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f81b345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbbcc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(history.history[\"loss\"][0:-2])\n",
    "plt.plot(history.history[\"val_loss\"][0:-2])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290f8e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weights(name, model):\n",
    "    weight_layer = model.get_layer(name)\n",
    "    weights = weight_layer.get_weights()[0]\n",
    "    weights = weights / np.linalg.norm(weights, axis = 1).reshape((-1, 1))\n",
    "    return weights\n",
    "\n",
    "anime_embeddings = extract_weights('anime_embedding', model)\n",
    "user_embeddings = extract_weights('user_embedding', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535e7403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "with open(os.path.join(CUR_INPUT, \"anime_embeddings.pickle\"), \"wb\") as f:\n",
    "    pickle.dump(anime_embeddings, f)\n",
    "\n",
    "with open(os.path.join(CUR_INPUT, \"user_embeddings.pickle\"), \"wb\") as f:\n",
    "    pickle.dump(user_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f5bc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(os.path.join(CUR_INPUT,\"anime_embeddings.tsv\"), anime_embeddings, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e6e779",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anime = pd.read_csv(os.path.join(CUR_INPUT, \"anime.csv\"), low_memory=True)\n",
    "df_anime = df_anime.rename(columns={\"MAL_ID\": \"anime_id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c1596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(CUR_INPUT, \"animelist_400+\", \"anime2anime_encoded.pickle\"), \"rb\") as input_file:\n",
    "    anime2anime_encoded = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435876eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anime[\"anime_id_mapped\"] = df_anime[\"anime_id\"].map(anime2anime_encoded, na_action=\"ignore\")\n",
    "df_anime = df_anime.dropna(subset=[\"anime_id_mapped\"])\n",
    "df_anime[\"anime_id_mapped\"] = df_anime[\"anime_id_mapped\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d679cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anime.sort_values(\"anime_id_mapped\")[[\"Name\"]].to_csv(os.path.join(CUR_INPUT,\"anime.tsv\"), sep=\"\\t\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518cba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anime.drop_duplicates(subset=[\"anime_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f469b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
